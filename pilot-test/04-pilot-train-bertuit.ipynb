{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "#from finetune_vs_scratch.preprocessing import special_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\"@usuario\", \"url\", \"hashtag\", \"emoji\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbc7144131f64940943d98903f91d6fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/562 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "AIDA-UPM/BERTuit-base does not appear to have a file named pytorch_model.bin but there is a file for TensorFlow weights. Use `from_tf=True` to load this model from those weights.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/home/darkstar/Workspace/ai/nlp/peruvian-slang-sentiment/pilot-test/04-pilot-train-bertuit.ipynb Cell 4\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/darkstar/Workspace/ai/nlp/peruvian-slang-sentiment/pilot-test/04-pilot-train-bertuit.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoModelForSequenceClassification\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/darkstar/Workspace/ai/nlp/peruvian-slang-sentiment/pilot-test/04-pilot-train-bertuit.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mAIDA-UPM/BERTuit-base\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/darkstar/Workspace/ai/nlp/peruvian-slang-sentiment/pilot-test/04-pilot-train-bertuit.ipynb#X25sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForSequenceClassification\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/darkstar/Workspace/ai/nlp/peruvian-slang-sentiment/pilot-test/04-pilot-train-bertuit.ipynb#X25sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     model_name,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/darkstar/Workspace/ai/nlp/peruvian-slang-sentiment/pilot-test/04-pilot-train-bertuit.ipynb#X25sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     num_labels\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/darkstar/Workspace/ai/nlp/peruvian-slang-sentiment/pilot-test/04-pilot-train-bertuit.ipynb#X25sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m )\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/darkstar/Workspace/ai/nlp/peruvian-slang-sentiment/pilot-test/04-pilot-train-bertuit.ipynb#X25sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/darkstar/Workspace/ai/nlp/peruvian-slang-sentiment/pilot-test/04-pilot-train-bertuit.ipynb#X25sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m tokenizer\u001b[39m.\u001b[39mmodel_max_length \u001b[39m=\u001b[39m \u001b[39m128\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:565\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    564\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 565\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    566\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    567\u001b[0m     )\n\u001b[1;32m    568\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    569\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:2950\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2944\u001b[0m has_file_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m   2945\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mrevision\u001b[39m\u001b[39m\"\u001b[39m: revision,\n\u001b[1;32m   2946\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mproxies\u001b[39m\u001b[39m\"\u001b[39m: proxies,\n\u001b[1;32m   2947\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtoken\u001b[39m\u001b[39m\"\u001b[39m: token,\n\u001b[1;32m   2948\u001b[0m }\n\u001b[1;32m   2949\u001b[0m \u001b[39mif\u001b[39;00m has_file(pretrained_model_name_or_path, TF2_WEIGHTS_NAME, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhas_file_kwargs):\n\u001b[0;32m-> 2950\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   2951\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m does not appear to have a file named\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2952\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m_add_variant(WEIGHTS_NAME,\u001b[39m \u001b[39mvariant)\u001b[39m}\u001b[39;00m\u001b[39m but there is a file for TensorFlow weights.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2953\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m Use `from_tf=True` to load this model from those weights.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2954\u001b[0m     )\n\u001b[1;32m   2955\u001b[0m \u001b[39melif\u001b[39;00m has_file(pretrained_model_name_or_path, FLAX_WEIGHTS_NAME, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhas_file_kwargs):\n\u001b[1;32m   2956\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   2957\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m does not appear to have a file named\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2958\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m_add_variant(WEIGHTS_NAME,\u001b[39m \u001b[39mvariant)\u001b[39m}\u001b[39;00m\u001b[39m but there is a file for Flax weights. Use\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2959\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m `from_flax=True` to load this model from those weights.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2960\u001b[0m     )\n",
      "\u001b[0;31mOSError\u001b[0m: AIDA-UPM/BERTuit-base does not appear to have a file named pytorch_model.bin but there is a file for TensorFlow weights. Use `from_tf=True` to load this model from those weights."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model_name = 'dccuchile/albert-base-spanish-finetuned-mldoc'\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=3\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.model_max_length = 128\n",
    "tokenizer.add_tokens(special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "\n",
    "def compute_metrics (eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis = -1)\n",
    "    \n",
    "    results = {}\n",
    "    results.update(f1_metric.compute(predictions=preds, references = labels, average=\"macro\"))\n",
    "    results.update(recall_metric.compute(predictions=preds, references = labels, average=\"macro\"))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\"train\": \"data/train.csv\", \"validation\": \"data/val.csv\", \"test\": \"data/test.csv\"}\n",
    "ds = load_dataset(\"csv\", data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'label_name', 'tokenized_text', 'sent_token_length', 'sent_bert_token_length', 'char_count', 'Character Count'],\n",
       "        num_rows: 573\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'label_name', 'tokenized_text', 'sent_token_length', 'sent_bert_token_length', 'char_count', 'Character Count'],\n",
       "        num_rows: 71\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'label_name', 'tokenized_text', 'sent_token_length', 'sent_bert_token_length', 'char_count', 'Character Count'],\n",
       "        num_rows: 64\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label to name\n",
    "def label2name(x):\n",
    "    if x == 0:\n",
    "        return \"Negative\"\n",
    "    if x == 1:\n",
    "        return \"Neutral\"\n",
    "    if x == 2:\n",
    "        return \"Positive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': Value(dtype='int64', id=None),\n",
       " 'label_name': Value(dtype='string', id=None),\n",
       " 'tokenized_text': Value(dtype='string', id=None),\n",
       " 'sent_token_length': Value(dtype='int64', id=None),\n",
       " 'sent_bert_token_length': Value(dtype='int64', id=None),\n",
       " 'char_count': Value(dtype='int64', id=None),\n",
       " 'Character Count': Value(dtype='int64', id=None)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysentimiento.preprocessing import preprocess_tweet\n",
    "preprocessed_ds = ds.map(lambda ex: {\"text\": preprocess_tweet(ex[\"text\"], lang=\"es\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08b9ef21a5804d8c83cb8079aa04d22e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/573 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9742915e4f96431e918fd0e6b8db0b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/71 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f633e63259e245169250af03a4b4fb17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/64 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_ds = preprocessed_ds.map(\n",
    "    lambda batch: tokenizer(\n",
    "        batch[\"text\"], padding=True, truncation=True\n",
    "        ),\n",
    "    batched=True, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=32,\n",
    "    output_dir=\"test_trainer\",\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=5,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8674662709236145, 'eval_f1': 0.2679012345679012, 'eval_recall': 0.3508771929824561, 'eval_runtime': 0.1552, 'eval_samples_per_second': 457.457, 'eval_steps_per_second': 57.987, 'epoch': 1.0}\n",
      "{'eval_loss': 0.7881259918212891, 'eval_f1': 0.4970588235294118, 'eval_recall': 0.5238095238095238, 'eval_runtime': 0.1709, 'eval_samples_per_second': 415.377, 'eval_steps_per_second': 52.653, 'epoch': 2.0}\n",
      "{'eval_loss': 1.0402268171310425, 'eval_f1': 0.6168179646440516, 'eval_recall': 0.5964912280701754, 'eval_runtime': 0.1555, 'eval_samples_per_second': 456.607, 'eval_steps_per_second': 57.88, 'epoch': 3.0}\n",
      "{'eval_loss': 0.9365906119346619, 'eval_f1': 0.6764227642276422, 'eval_recall': 0.6766917293233083, 'eval_runtime': 0.1644, 'eval_samples_per_second': 431.863, 'eval_steps_per_second': 54.743, 'epoch': 4.0}\n",
      "{'eval_loss': 1.0495860576629639, 'eval_f1': 0.6653945537666468, 'eval_recall': 0.6528822055137845, 'eval_runtime': 0.1471, 'eval_samples_per_second': 482.625, 'eval_steps_per_second': 61.178, 'epoch': 5.0}\n",
      "{'train_runtime': 25.4305, 'train_samples_per_second': 112.66, 'train_steps_per_second': 3.539, 'train_loss': 0.5045969645182292, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=90, training_loss=0.5045969645182292, metrics={'train_runtime': 25.4305, 'train_samples_per_second': 112.66, 'train_steps_per_second': 3.539, 'train_loss': 0.5045969645182292, 'epoch': 5.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.114953875541687, 'eval_f1': 0.6810143476810143, 'eval_recall': 0.671945701357466, 'eval_runtime': 0.1483, 'eval_samples_per_second': 431.596, 'eval_steps_per_second': 53.95, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.114953875541687,\n",
       " 'eval_f1': 0.6810143476810143,\n",
       " 'eval_recall': 0.671945701357466,\n",
       " 'eval_runtime': 0.1483,\n",
       " 'eval_samples_per_second': 431.596,\n",
       " 'eval_steps_per_second': 53.95,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(tokenized_ds[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
